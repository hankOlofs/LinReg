---
title: "The ridgereg function in the linreg package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ridgereg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(linreg)
library(caret)
library(mlbench)
```

The updated version of the `linreg` package provides a new function called `ridgereg()` that performs ridge regression. We are delighted to present the `ridgereg` function in this vignette and now we will show its functionality by an example. In the following example, we will create a predictive model for the `BostonHousing` data in the `mlbench` package. For the model training process we will use the `caret` package. Both packages are loaded in our workspace. 

Now let's get started! 

First, the `BostonHousing` data will be divided into a training set and a test set by using the
`caret` package, where $80\%$ will be for training, and the remaining $20\%$ for testing. The response variable is `medv` (median value of owner-occupied homes in USD 1000's).

```{r}
data("BostonHousing")
set.seed(123)
trainIndex <- createDataPartition(BostonHousing$medv, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- BostonHousing[ trainIndex,]
test_data  <- BostonHousing[-trainIndex,]
```
After dividing the data into a training and a test set, we will now fit two linear regression models to the `BostonHousing` training data using the `caret` package - one model consists of all continuous variables as predictors, and one model with only significant continuous variables using forward selection. Another package called `leaps` is required in order to perform forward selection. 

```{r}

library(leaps)
# Linear regression
lin <- train(medv ~ ., data = train_data, 
                 method = "lm")

# Linear Regression with Forward Selection
lin_forward <- train(medv ~ ., data = train_data, 
                 method = "leapForward",
                 tuneGrid = data.frame(nvmax = 1:(ncol(train_data)-1)))
```

```{r}
lin$results
lin_forward$results
# Best number of variables in model
nvmax <- lin_forward$bestTune
summary(lin_forward$finalModel)
coef(lin_forward$finalModel, unlist(nvmax))
```

*Evaluate the performance of this model on the training dataset*

Fit a ridge regression model using your ridgereg() function to the training dataset for different values of Î». How to include custom models in caret is described here http://topepo.github.io/caret/custom models.html.


```{r}
# Test to use custom models with lm
# testing with lm 
# Aim is to fit a ridge regression model using ridgereg() function to the training dataset for different values of lambda.

offset_mod <- getModelInfo("lm", regex = FALSE)[[1]]
offset_mod$fit <- function(x, y, wts, param, lev, last, classProbs, ...) {
  dat <- if(is.data.frame(x)) x else as.data.frame(x)
  dat$medv <- y
  lm(medv ~ crim + zn + indus + nox + rm + age + dis + rad + tax + ptratio + b + lstat, data = dat)
}

mod <- train(x = train, y = train_data$medv, method = offset_mod)
coef(mod$finalModel)

```


```{r}
# Do not include in vignette later.
#TESTING
library(MASS)
# Test ridge on mtcars - need to scale back?
# linreg package
ridgereg(formula = mpg ~ wt + cyl, data = mtcars, lambda = 1, QR = FALSE)

# MASS package
lm.ridge(formula = mpg ~ wt + cyl, data = mtcars, lambda = 1)

# stats package
lm(formula = mpg ~ wt + cyl, data = mtcars)

# caret package
ridge <- train(
  mpg ~ wt + cyl, data = mtcars, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 0, lambda = 1 )
)
coef(ridge$finalModel, ridge$bestTune$lambda)
```

```{r}
library(glmnet)
library(tidyverse)
lambda <- seq(0, 10, length = 100)

# Find the best hyperparameter value for lambda using 10-fold cross-validation on the training set.
# Need to replace with our ridgereg
ridge <- train(
  medv ~., data = train_data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 0, lambda = lambda )
)
coef(ridge$finalModel, ridge$bestTune$lambda)

predictions <- ridge %>% predict(test_data)

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test_data$medv),
  Rsquare = R2(predictions, test_data$medv)
)

# Predict
predictions <- ridge %>% predict(test_data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test_data$medv),
  Rsquare = R2(predictions, test_data$medv)
)


```


