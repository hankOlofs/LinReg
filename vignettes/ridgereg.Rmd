---
title: "The ridgereg function in the linreg package"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ridgereg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(linreg)
library(caret)
library(mlbench)
```

The updated version of the `linreg` package provides a new function called `ridgereg()` that performs ridge regression. We are delighted to present the `ridgereg` function in this vignette and now we will show its functionality by an example. In the following example, we will create a predictive model for the `BostonHousing` data in the `mlbench` package. For the model training process we will use the `caret` package. Both packages are loaded in our workspace. 

Now let's get started! 

First, the `BostonHousing` data will be divided into a training set and a test set by using the
`caret` package, where $80\%$ will be for training, and the remaining $20\%$ for testing. The response variable is `medv` (median value of owner-occupied homes in USD 1000's).

```{r}
data("BostonHousing")
set.seed(123)
trainIndex <- createDataPartition(BostonHousing$medv, p = .8, 
                                  list = FALSE, 
                                  times = 1)
train_data <- BostonHousing[ trainIndex,]
test_data  <- BostonHousing[-trainIndex,]
```
After dividing the data into a training and a test set, we will now fit two linear regression models to the `BostonHousing` training data using the `caret` package - one model consists of all continuous variables as predictors, and one model with only significant continuous variables using forward selection. Another package called `leaps` is required in order to perform forward selection. 

```{r}

library(leaps)
# Linear regression
lin <- train(medv ~ ., data = train_data, 
                 method = "lm")

# Linear Regression with Forward Selection
lin_forward <- train(medv ~ ., data = train_data, 
                 method = "leapForward",
                 tuneGrid = data.frame(nvmax = 1:(ncol(train_data)-1)))
```

```{r}
lin$results
lin_forward$results
# Best number of variables in model
nvmax <- lin_forward$bestTune
summary(lin_forward$finalModel)
coef(lin_forward$finalModel, unlist(nvmax))
```

*Evaluate the performance of this model on the training dataset*

Fit a ridge regression model using your ridgereg() function to the training dataset for different values of Î». How to include custom models in caret is described here http://topepo.github.io/caret/custom models.html.


```{r}
# Test to use custom models with lm
# testing with lm 
# Aim is to fit a ridge regression model using ridgereg() function to the training dataset for different values of lambda.

offset_mod <- getModelInfo("lm", regex = FALSE)[[1]]
offset_mod$fit <- function(x, y, wts, param, lev, last, classProbs, ...) {
  dat <- if(is.data.frame(x)) x else as.data.frame(x)
  dat$medv <- y
  lm(medv ~ crim + zn + indus + nox + rm + age + dis + rad + tax + ptratio + b + lstat, data = dat)
}

mod <- train(x = train, y = train_data$medv, method = offset_mod)
coef(mod$finalModel)

```


```{r}
# Do not include in vignette later.
#TESTING
library(MASS)
# Test ridge on mtcars - need to scale back?
# linreg package
ridgereg(formula = mpg ~ wt + cyl, data = mtcars, lambda = 1, QR = FALSE)

# MASS package
lm.ridge(formula = mpg ~ wt + cyl, data = mtcars, lambda = 1)

# stats package
lm(formula = mpg ~ wt + cyl, data = mtcars)

# caret package
ridge <- train(
  mpg ~ wt + cyl, data = mtcars, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 0, lambda = 1 )
)
coef(ridge$finalModel, ridge$bestTune$lambda)
```

A 10-fold cross-validation on the training set will be used in order to find the best hyperparameter value for $\lambda$. 

```{r}
library(glmnet)
library(tidyverse)
lambda <- seq(0, 10, length = 100)

# Find best lambda using 10-fold cross-validation
# Need to replace with our ridgereg
ridge <- train(
  medv ~., data = train_data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 0, lambda = lambda )
)
coef(ridge$finalModel, ridge$bestTune$lambda)

predictions <- ridge %>% predict(test_data)

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test_data$medv),
  Rsquare = R2(predictions, test_data$medv)
)

# Predict
predictions <- ridge %>% predict(test_data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test_data$medv),
  Rsquare = R2(predictions, test_data$medv)
)


```


```{r}

library(caret)
library(mlbench)
data(Sonar)

LogLoss <- function (data, lev = NULL, model = NULL){ 
  obs <- data[, "obs"] #truth
  cls <- levels(obs) #find class names
  probs <- data[, cls[2]] #use second class name to extract probs for 2nd clas
  probs <- pmax(pmin(as.numeric(probs), 1 - 1e-15), 1e-15) #bound probability, this line and bellow is just logloss calculation, irrelevant for your question 
  logPreds <- log(probs)        
  log1Preds <- log(1 - probs)
  real <- (as.numeric(data$obs) - 1)
  out <- c(mean(real * logPreds + (1 - real) * log1Preds)) * -1
  names(out) <- c("LogLoss") #important since this is specified in call to train. Output can be a named vector of multiple values. 
  out
}

fitControl <- trainControl(method = "cv",
                           number = 10,
                           summaryFunction = ridgereg)


fit <-  train(medv ~.,
             data = train_data,
             method = "rpart", 
             metric = "ridgereg" ,
             tuneLength = 10,
             trControl = fitControl,
             maximize = FALSE) #important, depending on calculated performance measure

fit


```




```{r}
# Include own model in train()
# PLEASE FIND WHAT'S WRONG
modelInfo <- list(label = "Hyperparameter",
                  library = "linreg",
                  type = "Regression",
                  parameters = data.frame(parameter = "lambda",
                  class = "numeric",
                  label = "Hyperparameter"),
                  grid = expand.grid(alpha = 0, lambda = seq(0, 10, length = 100)),
                  fit = function(x, y, wts, param, lev, last, classProbs, ...) {          
                    ## ridgereg requires a data frame with predictors and response
                    dat <- if(is.data.frame(x)) x else as.data.frame(x)
                    dat$medv <- y
                    mod <- ridgereg(
                      medv ~ .,
                      data = dat
                      )
                    },
                  predict = function(modelFit, newdata, submodels = NULL) {
                    if(!is.data.frame(newdata)) newdata <- as.data.frame(newdata)
                    ## By default a matrix is returned; we convert it to a vector
                    predict(modelFit, newdata)[,1]
                  },
                  loop = NULL,
                  prob = NULL,
                  levels = NULL)

## Just use the basic formula method so that these predictors
## are passed 'as-is' into the model fitting and prediction
## functions.
set.seed(307)
mboost_resamp <- train(medv ~ age, 
                       data = train, 
                       method = modelInfo,
                       trControl = trainControl(method = "repeatedcv",
                                                repeats = 10))
mboost_resamp
```


